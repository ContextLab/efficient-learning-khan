{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, sys, io, skvideo.io, argparse, math, datetime, ffmpy, shutil, wikipedia\n",
    "from google.cloud import videointelligence_v1 as videointelligence\n",
    "from google.cloud import vision\n",
    "from google.cloud import storage\n",
    "from google.cloud.vision import types\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extraction with google cloud video intelligence api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_labels(movie_to_process, bucket_name):\n",
    "    \n",
    "    #path to video input file and JSON output file\n",
    "    input_uri = 'gs://' + bucket_name + '/video_processing/' + movie_to_process\n",
    "    \n",
    "    \n",
    "    video_client = videointelligence.VideoIntelligenceServiceClient()\n",
    "    \n",
    "    # request extracted features\n",
    "    features = [videointelligence.enums.Feature.LABEL_DETECTION, videointelligence.enums.Feature.SPEECH_TRANSCRIPTION]\n",
    "\n",
    "    # request shot-level and frame_level labels, assign to label detection config\n",
    "    label_mode = videointelligence.enums.LabelDetectionMode.SHOT_AND_FRAME_MODE\n",
    "    label_config = videointelligence.types.LabelDetectionConfig(label_detection_mode=label_mode)\n",
    "    \n",
    "    # set language context to British english FOR SHERLOCK\n",
    "    speech_config = videointelligence.types.SpeechTranscriptionConfig(language_code='en-GB')\n",
    "    \n",
    "    # pass label detection and speech transcription configs to context \n",
    "    context = videointelligence.types.VideoContext(label_detection_config=label_config, SpeechTranscriptionConfig=speech_config)\n",
    "\n",
    "    # define analysis from parameters\n",
    "    operation = video_client.annotate_video(input_uri, features=features, video_context=context)\n",
    "    print('\\nprocessing video')\n",
    "    \n",
    "    # check if operation is done every 90s, return if done\n",
    "    result = operation.result(timeout=90)\n",
    "    print('\\nfinished processing video')\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# movie_to_process is sherlock_video.mp4\n",
    "# bucket is sherlock_movie\n",
    "\n",
    "# where do I want to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    # time offset from beginning of video where label is located\n",
    "    frame_offsets = []\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Process frame level label annotations\n",
    "    frame_labels = result.annotation_results[0].frame_label_annotations\n",
    "    for i, frame_label in enumerate(frame_labels):\n",
    "       for category_entity in frame_label.category_entities:\n",
    "            # look for categories that contain person regardless of situation\n",
    "            if (category_entity.description == 'person'):\n",
    "                # Each frame_label_annotation has many frames,\n",
    "                # but we keep information only about the first one\n",
    "                frame = frame_label.frames[0]\n",
    "                time_offset = (frame.time_offset.seconds +\n",
    "                               frame.time_offset.nanos / 1e9)\n",
    "                print('\\tFirst frame time offset: {}s'.format(time_offset))\n",
    "                print('\\tFirst frame confidence: {}'.format(frame.confidence))\n",
    "                print('\\n')\n",
    "                frame_offsets.append(time_offset)\n",
    "                \n",
    "    return(sorted(set(frame_offsets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scene details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Google Cloud Video Intelligence returning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## characters on screen (analogous to name - all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## camera angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indoor vs outdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## character speaking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## music presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## name - focus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## words on screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
