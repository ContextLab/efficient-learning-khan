{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:39:50.313201Z",
     "start_time": "2020-02-16T19:39:49.166770Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from os.path import join as opj\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T20:50:17.713250Z",
     "start_time": "2020-02-14T20:50:17.710127Z"
    }
   },
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:39:50.319151Z",
     "start_time": "2020-02-16T19:39:50.315806Z"
    }
   },
   "outputs": [],
   "source": [
    "datadir = '../../data/'\n",
    "rawdir = opj(datadir, 'raw')\n",
    "trajs_dir = opj(datadir, 'trajectories')\n",
    "models_dir = opj(datadir, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load lecture and question data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:39:50.332756Z",
     "start_time": "2020-02-16T19:39:50.322380Z"
    }
   },
   "outputs": [],
   "source": [
    "# Four Forces\n",
    "with open(opj(rawdir, 'forces_transcript_timestamped.txt'), 'r') as f:\n",
    "    ff_transcript = f.read()\n",
    "    \n",
    "# Birth of Stars\n",
    "with open(opj(rawdir, 'bos_transcript_timestamped.txt'), 'r') as f:\n",
    "    bos_transcript = f.read()\n",
    "    \n",
    "# quiz questions\n",
    "questions_df = pd.read_csv(opj(rawdir, 'questions.tsv'), sep='\\t', \n",
    "                           names=['index', 'lecture', 'question', \n",
    "                                  'ans_A', 'ans_B', 'ans_C', 'ans_D'], \n",
    "                           index_col='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:39:50.340027Z",
     "start_time": "2020-02-16T19:39:50.334824Z"
    }
   },
   "outputs": [],
   "source": [
    "# lecture transcript sliding window length\n",
    "lecture_wsize = 15\n",
    "# stop words corpus (see https://www.aclweb.org/anthology/W18-2502.pdf)\n",
    "stop_words = stopwords.words('english') + [\"let\", \"let's\", \"they'd\", \"they're\", \n",
    "                                           \"they've\", \"they'll\", \"that's\", \n",
    "                                           \"I'll\", \"I'm\"]\n",
    "# vectorizer params\n",
    "cv_params = {\n",
    "    'max_df': 0.95,\n",
    "    'min_df': 2,\n",
    "    'max_features': 500,\n",
    "    'stop_words': stop_words\n",
    "}\n",
    "\n",
    "# topic model params\n",
    "lda_params = {\n",
    "    'n_components': 20,\n",
    "    'learning_method': 'batch',\n",
    "    'random_state': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T21:05:36.850899Z",
     "start_time": "2020-02-14T21:05:36.829500Z"
    }
   },
   "source": [
    "## Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:39:50.346115Z",
     "start_time": "2020-02-16T19:39:50.341985Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_text(windows, sw=stop_words):\n",
    "    # some simple text preprocessing\n",
    "    clean_text = []\n",
    "    for chunk in windows:\n",
    "        no_punc = re.sub(\"[^a-zA-Z\\s'-]+\", '', chunk.lower()).replace('-', ' ')\n",
    "        no_stop = ' '.join([word for word in no_punc.split() if word not in sw])\n",
    "        clean = re.sub(\"'+\", '', no_stop)\n",
    "        clean_text.append(clean)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:39:50.356248Z",
     "start_time": "2020-02-16T19:39:50.348064Z"
    }
   },
   "outputs": [],
   "source": [
    "def _ts_to_secs(ts):\n",
    "    mins, secs = ts.split(':')\n",
    "    mins, secs = int(mins), int(secs)\n",
    "    return timedelta(minutes=mins, seconds=secs).total_seconds()\n",
    "    \n",
    "\n",
    "def parse_windows(transcript, wsize):\n",
    "    # formats lecture transcripts as overlapping sliding windows\n",
    "    # to feed as documents to topic model\n",
    "    # also returns timestamps of transcribed speech for interpolation\n",
    "    lines = transcript.splitlines()\n",
    "    text_lines = [l for ix, l in enumerate(lines) if ix % 2]\n",
    "    ts_lines = [_ts_to_secs(l) for ix, l in enumerate(lines) if not ix % 2]    \n",
    "    windows = []\n",
    "    timestamps = []\n",
    "    for ix in range(1, wsize):\n",
    "        start, end = 0, ix\n",
    "        windows.append(' '.join(text_lines[start : end]))\n",
    "        timestamps.append((ts_lines[start] + ts_lines[end - 1]) / 2)\n",
    "\n",
    "    for ix in range(len(ts_lines)):\n",
    "        start = ix\n",
    "        end = ix + wsize if ix + wsize <= len(text_lines) else len(text_lines)\n",
    "        windows.append(' '.join(text_lines[start : end]))\n",
    "        timestamps.append((ts_lines[start] + ts_lines[end - 1]) / 2)\n",
    "        \n",
    "    return windows, timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:39:50.361816Z",
     "start_time": "2020-02-16T19:39:50.358612Z"
    }
   },
   "outputs": [],
   "source": [
    "def interp_lecture(lec_traj, timestamps):\n",
    "    # interpolates lecture trajectories to 1 vector per second\n",
    "    new_tpts = np.arange(timestamps[-1])\n",
    "    interp_func = interp1d(timestamps, lec_traj, axis=0)\n",
    "    return interp_func(new_tpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process and reformat text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:39:50.479695Z",
     "start_time": "2020-02-16T19:39:50.365420Z"
    }
   },
   "outputs": [],
   "source": [
    "# get sliding windows & timestamps from lecture transcripts\n",
    "ff_windows, ff_timestamps = parse_windows(ff_transcript, lecture_wsize)\n",
    "bos_windows, bos_timestamps = parse_windows(bos_transcript, lecture_wsize)\n",
    "\n",
    "# remove punctuation, stop-words, digits, etc.\n",
    "ff_windows = format_text(ff_windows)\n",
    "bos_windows = format_text(bos_windows)\n",
    "\n",
    "# format quiz questions and correct answers\n",
    "grouped_qdf = questions_df.groupby('lecture')\n",
    "gen_qs, ff_qs, bos_qs = grouped_qdf['question'].apply(format_text)\n",
    "gen_correct, ff_correct, bos_correct = grouped_qdf['ans_A'].apply(format_text)\n",
    "all_qs = ff_qs + bos_qs + gen_qs\n",
    "all_ans_correct = ff_correct + bos_correct + gen_correct\n",
    "\n",
    "# format all answers\n",
    "all_ans = questions_df.loc[:, 'ans_A':].apply(format_text, axis=1).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T23:30:58.886025Z",
     "start_time": "2020-02-14T23:30:58.880720Z"
    }
   },
   "source": [
    "## Model lectures and quiz questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:39:51.763110Z",
     "start_time": "2020-02-16T19:39:50.481801Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create corpus\n",
    "corpus = ff_windows + bos_windows + all_qs + all_ans_correct\n",
    "\n",
    "# fit CountVectorizer model, vectorize corpus for fitting topic model\n",
    "tf_vectorizer = CountVectorizer(**cv_params).fit(corpus)\n",
    "corpus_tf = tf_vectorizer.transform(corpus)\n",
    "# vectorize lecture windows\n",
    "ff_lec_tf = tf_vectorizer.transform(ff_windows)\n",
    "bos_lec_tf = tf_vectorizer.transform(bos_windows)\n",
    "# vectorize questions\n",
    "ff_qs_tf = tf_vectorizer.transform(ff_qs)\n",
    "bos_qs_tf = tf_vectorizer.transform(bos_qs)\n",
    "gen_qs_tf = tf_vectorizer.transform(gen_qs)\n",
    "\n",
    "# fit LatentDirichletAllocation model\n",
    "lda = LatentDirichletAllocation(**lda_params).fit(corpus_tf)\n",
    "# transform lecture windows\n",
    "ff_traj = lda.transform(ff_lec_tf)\n",
    "bos_traj = lda.transform(bos_lec_tf)\n",
    "# transform questions\n",
    "ff_qs_vecs = lda.transform(ff_qs_tf)\n",
    "bos_qs_vecs = lda.transform(bos_qs_tf)\n",
    "gen_qs_vecs = lda.transform(gen_qs_tf)\n",
    "\n",
    "\n",
    "# interpolate lecture trajectories to 1 sample per second\n",
    "ff_traj = interp_lecture(ff_traj, ff_timestamps)\n",
    "bos_traj = interp_lecture(bos_traj, bos_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match question vectors to qIDs, model answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:39:51.801308Z",
     "start_time": "2020-02-16T19:39:51.765127Z"
    }
   },
   "outputs": [],
   "source": [
    "all_questions = dict.fromkeys(range(1, 40))\n",
    "all_answers = dict.fromkeys(all_questions.keys())\n",
    "qs_concat = np.concatenate((ff_qs_vecs, bos_qs_vecs, gen_qs_vecs))\n",
    "for qID, q_vec in enumerate(qs_concat, start=1):\n",
    "    all_questions[qID] = q_vec\n",
    "    all_answers[qID] = lda.transform(tf_vectorizer.transform(all_ans[qID]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:52:54.494287Z",
     "start_time": "2020-02-16T17:52:54.387881Z"
    }
   },
   "source": [
    "## Save trajectories and fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:39:51.815723Z",
     "start_time": "2020-02-16T19:39:51.802847Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.save(opj(trajs_dir, 'forces_lecture'), ff_traj)\n",
    "# np.save(opj(trajs_dir, 'bos_lecture'), ff_traj)\n",
    "# np.save(opj(trajs_dir, 'forces_questions'), ff_qs_vecs)\n",
    "# np.save(opj(trajs_dir, 'bos_questions'), bos_qs_vecs)\n",
    "# np.save(opj(trajs_dir, 'general_questions'), gen_qs_vecs)\n",
    "\n",
    "# with open(opj(trajs_dir, 'all_questions.p'), 'wb') as f:\n",
    "#     pickle.dump(all_questions, f)\n",
    "# with open(opj(trajs_dir, 'all_answers.p'), 'wb') as f:\n",
    "#     pickle.dump(all_answers, f)    \n",
    "\n",
    "# np.save(opj(models_dir, 'fit_CV'), tf_vectorizer)\n",
    "# np.save(opj(models_dir, 'fit_LDA'), lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
