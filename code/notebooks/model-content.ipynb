{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T18:00:25.592436Z",
     "start_time": "2020-02-16T18:00:22.720196Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hypertools as hyp\n",
    "from datetime import timedelta\n",
    "from os.path import join as opj\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import entropy\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T20:50:17.713250Z",
     "start_time": "2020-02-14T20:50:17.710127Z"
    }
   },
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T18:04:51.903360Z",
     "start_time": "2020-02-16T18:04:51.900129Z"
    }
   },
   "outputs": [],
   "source": [
    "datadir = '../../data/'\n",
    "rawdir = opj(datadir, 'raw')\n",
    "trajs_dir = opj(datadir, 'trajectories')\n",
    "models_dir = opj(datadir, 'models')\n",
    "\n",
    "figdir = '../../figures/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load lecture and question data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T18:00:25.616839Z",
     "start_time": "2020-02-16T18:00:25.604308Z"
    }
   },
   "outputs": [],
   "source": [
    "# Four Forces\n",
    "with open(opj(rawdir, 'forces_transcript_timestamped.txt'), 'r') as f:\n",
    "    ff_transcript = f.read()\n",
    "    \n",
    "# Birth of Stars\n",
    "with open(opj(rawdir, 'bos_transcript_timestamped.txt'), 'r') as f:\n",
    "    bos_transcript = f.read()\n",
    "    \n",
    "# quiz questions\n",
    "questions_df = pd.read_csv(opj(rawdir, 'questions.tsv'), sep='\\t', \n",
    "                           names=['index', 'lecture', 'question', \n",
    "                                  'ans_A', 'ans_B', 'ans_C', 'ans_D'], \n",
    "                           index_col='index')\n",
    "\n",
    "# quiz scores\n",
    "scores_df = pd.read_csv(opj(rawdir, 'Graded_results_19f_49.csv'), index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T18:00:25.625103Z",
     "start_time": "2020-02-16T18:00:25.618857Z"
    }
   },
   "outputs": [],
   "source": [
    "# lecture transcript sliding window length\n",
    "lecture_wsize = 15\n",
    "# stop words corpus (see https://www.aclweb.org/anthology/W18-2502.pdf)\n",
    "stop_words = stopwords.words('english') + [\"let\", \"let's\", \"they'd\", \"they're\", \n",
    "                                           \"they've\", \"they'll\", \"that's\", \n",
    "                                           \"I'll\", \"I'm\"]\n",
    "# vectorizer params\n",
    "cv_params = {\n",
    "    'max_df': 0.95,\n",
    "    'min_df': 2,\n",
    "    'max_features': 500,\n",
    "    'stop_words': stop_words\n",
    "}\n",
    "\n",
    "# topic model params\n",
    "lda_params = {\n",
    "    'n_components': 20,\n",
    "    'learning_method': 'batch',\n",
    "    'random_state': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T21:05:36.850899Z",
     "start_time": "2020-02-14T21:05:36.829500Z"
    }
   },
   "source": [
    "## Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T18:00:25.632356Z",
     "start_time": "2020-02-16T18:00:25.627117Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_text(windows, sw=stop_words):\n",
    "    # some simple text preprocessing\n",
    "    clean_text = []\n",
    "    for chunk in windows:\n",
    "        no_punc = re.sub(\"[^a-zA-Z\\s'-]+\", '', chunk.lower()).replace('-', ' ')\n",
    "        no_stop = ' '.join([word for word in no_punc.split() if word not in sw])\n",
    "        clean = re.sub(\"'+\", '', no_stop)\n",
    "        if clean:\n",
    "            clean_text.append(clean)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T18:00:25.642598Z",
     "start_time": "2020-02-16T18:00:25.634943Z"
    }
   },
   "outputs": [],
   "source": [
    "def _ts_to_secs(ts):\n",
    "    mins, secs = ts.split(':')\n",
    "    mins, secs = int(mins), int(secs)\n",
    "    return timedelta(minutes=mins, seconds=secs).total_seconds()\n",
    "    \n",
    "\n",
    "def parse_windows(transcript, wsize):\n",
    "    # formats lecture transcripts as overlapping sliding windows\n",
    "    # to feed as documents to topic model\n",
    "    # also returns timestamps of transcribed speech for interpolation\n",
    "    lines = transcript.splitlines()\n",
    "    text_lines = [l for ix, l in enumerate(lines) if ix % 2]\n",
    "    ts_lines = [_ts_to_secs(l) for ix, l in enumerate(lines) if not ix % 2]    \n",
    "    windows = []\n",
    "    timestamps = []\n",
    "    for ix in range(1, wsize):\n",
    "        start, end = 0, ix\n",
    "        windows.append(' '.join(text_lines[start : end]))\n",
    "        timestamps.append((ts_lines[start] + ts_lines[end - 1]) / 2)\n",
    "\n",
    "    for ix in range(len(ts_lines)):\n",
    "        start = ix\n",
    "        end = ix + wsize if ix + wsize <= len(text_lines) else len(text_lines)\n",
    "        windows.append(' '.join(text_lines[start : end]))\n",
    "        timestamps.append((ts_lines[start] + ts_lines[end - 1]) / 2)\n",
    "        \n",
    "    return windows, timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T18:00:25.649722Z",
     "start_time": "2020-02-16T18:00:25.645997Z"
    }
   },
   "outputs": [],
   "source": [
    "def interp_lecture(lec_traj, timestamps):\n",
    "    # interpolates lecture trajectories to 1 vector per second\n",
    "    new_tpts = np.arange(timestamps[-1])\n",
    "    interp_func = interp1d(timestamps, lec_traj, axis=0)\n",
    "    return interp_func(new_tpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process and reformat text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T18:00:25.767654Z",
     "start_time": "2020-02-16T18:00:25.663588Z"
    }
   },
   "outputs": [],
   "source": [
    "# get sliding windows & timestamps from lecture transcripts\n",
    "ff_windows, ff_timestamps = parse_windows(ff_transcript, lecture_wsize)\n",
    "bos_windows, bos_timestamps = parse_windows(bos_transcript, lecture_wsize)\n",
    "\n",
    "# remove punctuation, stop-words, digits, etc.\n",
    "ff_windows = format_text(ff_windows)\n",
    "bos_windows = format_text(bos_windows)\n",
    "\n",
    "# format quiz questions and correct answers\n",
    "grouped_qdf = questions_df.groupby('lecture')\n",
    "gen_qs, ff_qs, bos_qs = grouped_qdf['question'].apply(format_text)\n",
    "gen_correct, ff_correct, bos_correct = grouped_qdf['ans_A'].apply(format_text)\n",
    "all_qs = ff_qs + bos_qs + gen_qs\n",
    "all_ans_correct = ff_correct + bos_correct + gen_correct\n",
    "\n",
    "# # format all answers\n",
    "# ISSUE: TRACK CORRECT COLUMN OF ANSWER WHERE SOME ANSWERS LEAVE NO TEXT AFTER FORMAT_TEXT IS APPLIED\n",
    "# all_ans = [None, None, None]\n",
    "# for lec_ix in range(3):\n",
    "#     grp = grouped_qdf['ans_A', 'ans_B', 'ans_C', 'ans_D'].get_group(lec_ix)\n",
    "#     all_ans[lec_ix] = grp.apply(format_text, axis=1).to_dict()\n",
    "# gen_ans, ff_ans, bos_ans = all_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T23:30:58.886025Z",
     "start_time": "2020-02-14T23:30:58.880720Z"
    }
   },
   "source": [
    "## Model lectures and questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T18:00:27.161782Z",
     "start_time": "2020-02-16T18:00:25.769227Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create corpus\n",
    "corpus = ff_windows + bos_windows + all_qs + all_ans_correct\n",
    "\n",
    "# fit CountVectorizer model, vectorize corpus for fitting topic model\n",
    "tf_vectorizer = CountVectorizer(**cv_params).fit(corpus)\n",
    "corpus_tf = tf_vectorizer.transform(corpus)\n",
    "# vectorize lecture windows\n",
    "ff_lec_tf = tf_vectorizer.transform(ff_windows)\n",
    "bos_lec_tf = tf_vectorizer.transform(bos_windows)\n",
    "# vectorize questions\n",
    "ff_qs_tf = tf_vectorizer.transform(ff_qs)\n",
    "bos_qs_tf = tf_vectorizer.transform(bos_qs)\n",
    "gen_qs_tf = tf_vectorizer.transform(gen_qs)\n",
    "\n",
    "# fit LatentDirichletAllocation model\n",
    "lda = LatentDirichletAllocation(**lda_params).fit(corpus_tf)\n",
    "# transform lecture windows\n",
    "ff_traj = lda.transform(ff_lec_tf)\n",
    "bos_traj = lda.transform(bos_lec_tf)\n",
    "# transform questions\n",
    "ff_qs_vecs = lda.transform(ff_qs_tf)\n",
    "bos_qs_vecs = lda.transform(bos_qs_tf)\n",
    "gen_qs_vecs = lda.transform(gen_qs_tf)\n",
    "\n",
    "# interpolate lecture trajectories to 1 sample per second\n",
    "ff_traj = interp_lecture(ff_traj, ff_timestamps)\n",
    "bos_traj = interp_lecture(bos_traj, bos_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:52:54.494287Z",
     "start_time": "2020-02-16T17:52:54.387881Z"
    }
   },
   "source": [
    "## Save trajectories and fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T18:00:27.174121Z",
     "start_time": "2020-02-16T18:00:27.164019Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.save(opj(trajs_dir, 'forces_lecture'), ff_traj)\n",
    "# np.save(opj(trajs_dir, 'bos_lecture'), ff_traj)\n",
    "# np.save(opj(trajs_dir, 'forces_questions'), ff_qs_vecs)\n",
    "# np.save(opj(trajs_dir, 'bos_questions'), bos_qs_vecs)\n",
    "# np.save(opj(trajs_dir, 'general_questions'), gen_qs_vecs)\n",
    "\n",
    "# np.save(opj(models_dir, 'fit_CV'), tf_vectorizer)\n",
    "# np.save(opj(models_dir, 'fit_LDA'), lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
