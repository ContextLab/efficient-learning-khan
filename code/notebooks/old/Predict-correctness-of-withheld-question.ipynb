{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Correctness of Withheld Question\n",
    "\n",
    "A major goal of this project has been to show that placing questions in topic space provides us with a richer picture of a person's knowledge than simply reporting the fraction of questions the person answered correctly. One way of showing this is to demonstrate that, if we take into account the questions' positions in topic space, we're able to predict the correctness of the participant's response to a held-out question with greater accuracy than if we make our prediction considering only the percentage of the other (non-held-out) questions the participant answered correctly.\n",
    "\n",
    "Below is my attempt to make such a comparison.\n",
    "\n",
    "-Will Baxley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the locations of some relevant directories\n",
    "vid_transc_dir = '../video transcript analysis'\n",
    "answers_dir = '../graded_answers/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>correct?</th>\n",
       "      <th>participantID</th>\n",
       "      <th>qID</th>\n",
       "      <th>set</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  correct?  participantID  qID  set  video\n",
       "0           0         0             40   29    0      2\n",
       "1           1         0             40   31    0      0\n",
       "2           2         0             40   17    0      2\n",
       "3           3         1             40   36    0      0\n",
       "4           4         0             40    7    0      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the graded answers\n",
    "results = pd.read_csv(os.path.join(answers_dir, \"Graded_results_19f_49.csv\"))\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine our accuracy at predicting held-out questions using only each individual's percentage correct\n",
    "For each participant, hold out each question, one at a time, and try to predict whether the held-out question was answered correctly or incorrectly by considering the participants' average correctness across the other (non-held-out) questions. Report the total accuracy of all such preductions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_own_accuracy(data):\n",
    "    # the numerator and demonimator of the accuracy term we'll return\n",
    "    correct = total = 0\n",
    "    \n",
    "    # iterate across each participant\n",
    "    for participant in data[\"participantID\"].unique():\n",
    "        \n",
    "        # filter the data to include only our given participant\n",
    "        participant_responses = data[data[\"participantID\"] == participant]\n",
    "        \n",
    "        # iterate across each question the participant answered (in the given section)\n",
    "        for question in participant_responses[\"qID\"]:\n",
    "            \n",
    "            # the participant's percentage correct across all questions except the held-out one \n",
    "            avg_correct = np.mean(participant_responses[participant_responses[\"qID\"] != question][\"correct?\"])\n",
    "            \n",
    "            # if avg_correct is at least 50%, guess correct; otherwise, guess incorrect\n",
    "            predicted_response = 1 if (avg_correct >= 0.5) else 0\n",
    "            \n",
    "            # the true correctness of the held-out question\n",
    "            true_response = int(participant_responses[participant_responses[\"qID\"] == question][\"correct?\"])\n",
    "            \n",
    "            # update correct and total\n",
    "            if predicted_response == true_response:\n",
    "                correct += 1\n",
    "            \n",
    "            total += 1\n",
    "    \n",
    "    # return the total accuracy\n",
    "    return correct / total "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break the data into three sections: (0) questions before video 1, (1) questions between video 1 and 2, and (2) questions after video 2\n",
    "We expect participants' knowledge to change after watching each video since, presumably, they're learning. We see, after all, that across all participants, the accuracy in answering questions increases from ~46% to ~78% from before video 1 (section 0) to after video 2 (section 2). As a result, I think it makes sense to consider each of these sections separately; in other words, to say that each participant has a unique \"knowledge\" - whatever that is - that differs across the three sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "set0_results = results[results[\"set\"] == 0]\n",
    "set1_results = results[results[\"set\"] == 1]\n",
    "set2_results = results[results[\"set\"] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the average percencentage correct in each section. Any model that doesn't predict the correctness of held-out questions with at least this accuracy is worse than the naive always-guess-correct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4553846153846154"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set0_results[\"correct?\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6030769230769231"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set1_results[\"correct?\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7846153846153846"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set2_results[\"correct?\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Within each section, across all participants, report the accuracy of predicting success at held-out question\n",
    "\n",
    "By \"accuracy\", I mean (number of correct predictions) / (number of total predictions). Other performance metrics might be more appropriate, but I chose this one as a first attempt because it's simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5707692307692308"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_own_accuracy(set0_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5707692307692308"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_own_accuracy(set1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7892307692307692"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_own_accuracy(set2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine our accuracy at predicting held-out questions using all other participants' success at those questions\n",
    "This calculation is a bit tangential to the main point of this notebook, but it's something I was curious about. For each participant, consider each question and predict whether or not it was answered correctly by considering how successful the *other* participants were, on average, at answering that question. Again, report the total accuracy of all such predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_others_accuracy(data):\n",
    "    # for determining accuracy\n",
    "    correct = total = 0\n",
    "    \n",
    "    # iterate across each participant\n",
    "    for participant in data[\"participantID\"].unique():\n",
    "        \n",
    "        # 1. compute the percentage correct on each question, excluding the responses of the given participant\n",
    "        avg_response = data[data[\"participantID\"] != participant].groupby(\"qID\").mean()[\"correct?\"]\n",
    "        \n",
    "        # 2. for each question the participant answered, try to guess if it will be correct or not\n",
    "        participant_responses = data[data[\"participantID\"] == participant]  # the responses of our given participant\n",
    "        \n",
    "        for question in participant_responses[\"qID\"]:\n",
    "            guess = 1 if (avg_response[question] >= 0.5) else 0\n",
    "            \n",
    "            true_response = int(participant_responses[participant_responses[\"qID\"] == question][\"correct?\"])\n",
    "            \n",
    "            if guess == true_response:\n",
    "                correct += 1\n",
    "            \n",
    "            total += 1\n",
    "    \n",
    "    return correct / total "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Within each section, across all participants, report the accuracy of predicting success at held-out question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6507692307692308"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_others_accuracy(set0_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7215384615384616"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_others_accuracy(set1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8046153846153846"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_others_accuracy(set2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now fit some topic models\n",
    "I took much of the code in this section from predict-knowledge-topic-space.inpyb. Refer to it for questions about the choices I've made (e.g. what text I used to fit the model, what stopwords I used, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the video transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four Forces transcript, diced via a sliding window\n",
    "forces_video_df = pd.read_csv(os.path.join(vid_transc_dir,'fourforcesdiced.tsv'), \n",
    "                            error_bad_lines=False, header=None, sep='\\t', usecols=[0])\n",
    "forces_video_samples = forces_video_df[0].tolist()\n",
    "\n",
    "# Birth of Stars transcript, diced via a sliding window\n",
    "bos_video_df = pd.read_csv(os.path.join(vid_transc_dir, 'birthofstarsdiced.tsv'), \n",
    "                            error_bad_lines=False, header=None, sep='\\t', usecols=[0])\n",
    "bos_video_samples = pd.Series(bos_video_df[0]).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the questions as a dataframe\n",
    "questions_df = pd.read_csv('../data analysis/astronomyquestions.tsv', sep='\\t', \n",
    "            names=['index', 'video', 'question', 'ans_A', 'ans_B', 'ans_C', 'ans_D'], index_col='index') \n",
    "\n",
    "# organize question by type (FFs, BoS, general)\n",
    "forces_questions_samples = questions_df.loc[questions_df.video == 1].question.tolist()\n",
    "bos_questions_samples = questions_df.loc[questions_df.video == 2].question.tolist()\n",
    "general_question_samples = questions_df.loc[questions_df.video == 0].question.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords and punctuation, convert to lowercase, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = stopwords.words('english') + [\"let's\", \"they'd\", \"they're\", \"they've\", \"they'll\", \"that's\", \n",
    "                                              \"I'll\", \"I'm\"]\n",
    "\n",
    "def format_text(text):\n",
    "    \"\"\"\n",
    "    Function to format documents for tokenization and modeling\n",
    "    \"\"\"\n",
    "    \n",
    "    clean_text = []\n",
    "    \n",
    "    for sentence in text:\n",
    "        no_punc = re.sub(\"[^a-zA-Z\\s']+\", '', sentence.lower())\n",
    "        no_stop = ' '.join([word for word in no_punc.split() if word not in all_stopwords])\n",
    "        clean = re.sub(\"'+\", '', no_stop)\n",
    "        clean_text.append(clean)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format lecture and question text\n",
    "fvs_formatted = format_text(forces_video_samples)\n",
    "bvs_formatted = format_text(bos_video_samples)\n",
    "fqs_formatted = format_text(forces_questions_samples)\n",
    "bqs_formatted = format_text(bos_questions_samples)\n",
    "gqs_formatted = format_text(general_question_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize some topic modelling parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_params = {\n",
    "    'max_df': 0.95,\n",
    "    'min_df': 2,\n",
    "    'max_features': 500,\n",
    "    'stop_words': 'english'\n",
    "}\n",
    "\n",
    "lda_params = {\n",
    "    'n_components': 12,\n",
    "    'max_iter': 10,\n",
    "    'learning_method': 'online',\n",
    "    'learning_offset':50.,\n",
    "    'random_state': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize count vectorizer\n",
    "tf_vectorizer = CountVectorizer(**vec_params)\n",
    "\n",
    "# fit to both lectures and all questions\n",
    "tf_vectorizer.fit(fvs_formatted + fqs_formatted\n",
    "                  + bvs_formatted + bqs_formatted)\n",
    "\n",
    "# transform question samples\n",
    "forces_questions_tf = tf_vectorizer.transform(fqs_formatted)\n",
    "bos_questions_tf = tf_vectorizer.transform(bqs_formatted)\n",
    "general_questions_tf = tf_vectorizer.transform(gqs_formatted)\n",
    "\n",
    "# vectorize the entire corpus (both video transcripts and the related questions)\n",
    "all_tf = tf_vectorizer.transform(fvs_formatted + fqs_formatted + bvs_formatted + bqs_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LDA model, fit to both lectures and lecture-related questions\n",
    "lda = LatentDirichletAllocation(**lda_params)\n",
    "lda.fit(all_tf)\n",
    "\n",
    "# transform questions\n",
    "forces_q_traj = lda.transform(forces_questions_tf)\n",
    "bos_q_traj = lda.transform(bos_questions_tf)\n",
    "general_q_traj = lda.transform(general_questions_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reorganize the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the question trajectories into a single list\n",
    "all_q_traj = list(forces_q_traj) + list(bos_q_traj) + list(general_q_traj)\n",
    "\n",
    "# insert an empty list at the 0th position so that trajectories can be indexed from 1 to 39 (i.e. by qID)\n",
    "all_q_traj.insert(0, np.ndarray([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A method for determining how close two vectors are in space\n",
    "I was looking for some metric that's higher when two vectors are closer and lower when they're further appart. I chose this method (inverse of euclidean distance) because it's simple, but I'm sure there are many other possible ways of doing this, and I'd be open to suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given two equi-dimensional vectors a and b (in np.ndarray form), return 1 / (euclidean_dist(a, b))\n",
    "def inverse_euclidean_distance(a, b):\n",
    "    # compute the euclidean distance between the two vectors\n",
    "    dist = np.linalg.norm(a-b)\n",
    "    \n",
    "    # if the distance is 0, return an arbitrary large number\n",
    "    if dist == 0:\n",
    "        print(\"bet you didn't expect that to happen, did you?\")\n",
    "        return 10^6\n",
    "    \n",
    "    # otherwise, return 1 / dist\n",
    "    return 1 / dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine our accuracy at predicting held-out questions using their proximity in topic space to other questions answered by the participant\n",
    "For each participant, for each held-out question, compute the participant's \"knowledge\" at the point of the held-out question by summing the \"closeness\" to each of the correctly answered questions and subtracting the \"closeness\" to each incorrectly answered question. Predict correct when knowledge >= 0, otherwise incorrect, and report the total accuracy of all such predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_own_knowledge(data, trajs, closeness_metric):\n",
    "    # for determining accuracy\n",
    "    correct = total = 0\n",
    "    \n",
    "    # iterate across each participant\n",
    "    for participant in data[\"participantID\"].unique():\n",
    "        \n",
    "        # get the data relating only to that one participant\n",
    "        participant_responses = data[data[\"participantID\"] == participant]\n",
    "        \n",
    "        # withhold each question, one at a time, and see how well we can predict it\n",
    "        for qID in participant_responses[\"qID\"]:\n",
    "            \n",
    "            # all the questions answered by the participant except the withheld one\n",
    "            other_qIDs = [q for q in participant_responses[\"qID\"] if q is not qID]\n",
    "            \n",
    "            # for each question in other_qIDs, a record of whether is was answered correctly\n",
    "            q_correctness = [int(participant_responses[participant_responses[\"qID\"] == q][\"correct?\"]) for q in other_qIDs]\n",
    "            \n",
    "            # tranform terms in q_correcness from the set {0,1} to {-1,1}\n",
    "            q_correctness = (np.array(q_correctness) * 2) - 1\n",
    "            \n",
    "            # determine the magnitude of the individual's knowledge at the location of the withheld question\n",
    "            knowledge = np.sum([c * closeness_metric(trajs[qID], trajs[q]) for q,c in zip(other_qIDs, q_correctness)])\n",
    "            \n",
    "            # our prediction of how the participant answered the given question\n",
    "            predicted_response = 1 if (knowledge >= 0) else 0\n",
    "            \n",
    "            # how the participant actually answered the withheld question\n",
    "            true_response = int(participant_responses[participant_responses[\"qID\"] == qID][\"correct?\"])\n",
    "            \n",
    "            # update correct and total\n",
    "            if predicted_response == true_response:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Within each section, across all participants, report the accuracy of predicting success at held-out question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5784615384615385"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_own_knowledge(set0_results, all_q_traj, inverse_euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_own_knowledge(set1_results, all_q_traj, inverse_euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7415384615384616"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_own_knowledge(set2_results, all_q_traj, inverse_euclidean_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat, but this time use correlation as a measure of closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.563076923076923"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_own_knowledge(set0_results, all_q_traj, np.correlate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6230769230769231"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_own_knowledge(set1_results, all_q_traj, np.correlate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7492307692307693"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_own_knowledge(set2_results, all_q_traj, np.correlate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "These results show that our accuracy at predicting the correctness of a held-out question is similar when we (1) consider the participant's accuracy on the other questions and (2) when we instead consider the relashionship between the position of the held-out question and the non-held-out questions in topic space. This is a bit disappointing, but of course there are many ways this procedure could be modified, and some of those might lead to better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
